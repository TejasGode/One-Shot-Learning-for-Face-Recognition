{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "One Shot Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11AXncHGa4cSGS__A5qJ8GUFWiRDHvKBG",
      "authorship_tag": "ABX9TyMxQ06Fn+Th5Cv5J1Rfc75d"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYndJcXgGa1n",
        "colab_type": "text"
      },
      "source": [
        "One Shot Learning - Facial Recognition\n",
        "\n",
        "We will use Siamese Network, which does not require huge data to train.\n",
        "\n",
        "Siamese Network consists of two Symmetrical Neural Networks having same weights and architectures. And at last they are joined together (at end ) which acts as distance function.\n",
        "\n",
        "FaceNet by Google : https://arxiv.org/abs/1503.03832\n",
        "\n",
        "Idea here is that, Both the networks that return embedding vectors. \n",
        " Two embeddings belonging to the same person should have a lower distance, while embeddings belonging to different person’s image should show a larger distance.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_CgtcRBHHx9",
        "colab_type": "text"
      },
      "source": [
        "FaceNet Network\n",
        "\n",
        "Here we are using encodings of anchor, positive, negative images of shape (None, 128) vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4BcgBAsHETW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "import time\n",
        "from multiprocessing.dummy import Pool\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "import tensorflow as tf\n",
        "from fr_utils import *\n",
        "from inception_blocks_v2 import *\n",
        "\n",
        "\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "model = faceRecoModel(input_shape=(3,96,96))\n",
        "\n",
        "def triplet_loss(y_true, y_pred, alpha=0.3):\n",
        "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
        "\n",
        "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1) \n",
        "\n",
        "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1)\n",
        "\n",
        "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
        "    \n",
        "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n",
        "    \n",
        "    return loss\n",
        "\n",
        "model.compile(optimizer='adam', loss= triplet_loss, metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk2-km8JRrAJ",
        "colab_type": "text"
      },
      "source": [
        "Image Embeddings from pre-trained FaceNet mode\n",
        "when given a picture of a face, will extract high-quality features from it and predict a 128-element vector representation of these features, called a face embedding.\n",
        "\n",
        "Standard Deviation, .std() is measured as the spread of data distribution in the given data set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPy8EwQpdvxB",
        "colab_type": "text"
      },
      "source": [
        "Preparing Dataset - Mapping person’s name to its embedding\n",
        "\n",
        "\n",
        "Keep in mind that all the images that are fed to network must have 96*96 pixel images.\n",
        "\n",
        "We are using,\n",
        "**fr_utils** as a functions to feed images to the network and getting the encoding of images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HJFb0RlJC5l",
        "colab_type": "text"
      },
      "source": [
        "For importing python(.py) files:\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.append('/content/drive/My Drive/')\n",
        "\n",
        "import fr_utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgZvsLW-cwbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "face_database = {}\t\t# Dictionary having image name and corresponding embeddings of the image\n",
        "\n",
        "path = \"..path_to_your_folder_where_images_are_stored..\"\n",
        "\n",
        "for image in os.listdir(path):\n",
        "\n",
        "\t\tprint('1st: ',os.path.join(path,image))\n",
        "\t\n",
        "\t\tidentity = os.path.splitext(os.path.basename(image))[0]\n",
        "\t\tprint('splitext: ',identity) \n",
        "\t\n",
        "\t\tface_database[identity] = img_path_to_encoding(os.path.join(path,image), model)\n",
        "\n",
        "print(face_database)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDnuYFJI4Nhq",
        "colab_type": "text"
      },
      "source": [
        "Facial Recognition\n",
        "\n",
        "As we have our face database ready to use.\n",
        "Here we will compute L2 distance  between target image and the avialable database.\n",
        "\n",
        "For more about **norm** check this: https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.5-Norms/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdEF21b-VNAE",
        "colab_type": "text"
      },
      "source": [
        "Using only face instead of whole image for recognition problems help in better way. \n",
        "So, we will use OpenCV. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw07zJD2rUCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import pnslib\n",
        "import sys\n",
        "\n",
        "path = \"..path_to_image.../img.jpg\"\n",
        "image = cv2.imread(path)\n",
        "\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "#haarcascade_frontalface_default.xml needs to be downloaded\n",
        "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "faces = faceCascade.detectMultiScale(\n",
        "    gray,\n",
        "    scaleFactor= 1.3,\n",
        "    minNeighbors= 3,\n",
        "    minSize=(30, 30)\n",
        ")\n",
        "print(\"[INFO] Found {0} Faces.\".format(len(faces)))\n",
        "\n",
        "for (x, y, w, h) in faces:\n",
        "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "    roi_img = image[y:y + h, x:x + w]\n",
        "    print(\"[INFO] Object found. Saving ...\")\n",
        "    cv2.imwrite(str(w) + str(h) + '_IMG.jpg', roi_img)\n",
        "\n",
        "status = cv2.imwrite('..path_where_you_want_to_save_roi_image../img.jpg', roi_img)\n",
        "\n",
        "print(\"[INFO] Image faces_detected.jpg written to filesystem: \", status)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mlg5Ef_TonM",
        "colab_type": "text"
      },
      "source": [
        "Identify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxWtSpTC3kGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import linalg as LA\n",
        "\n",
        "def recog(target, database, model): \n",
        "    # target is the image that we want to recognize\n",
        "    encoding = img_path_to_encoding(path, model)\n",
        "\n",
        "    min_dist =  1.0\n",
        "    identity = None\n",
        "\n",
        "    for (name, db_enc) in database.items():\n",
        "        \n",
        "        dist = LA.norm(db_enc - encoding)\n",
        "        \n",
        "        print(\"Distance for %s is %s \" %(name, dist))\n",
        "\n",
        "        if dist < min_dist:\n",
        "            min_dist = dist\n",
        "            identity = name\n",
        "\n",
        "    if min_dist > 0.07:                                 \n",
        "        return \"Unknown\"\n",
        "\n",
        "    else:\n",
        "        return (identity)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKTT4NRBUHc7",
        "colab_type": "text"
      },
      "source": [
        "Here 0.07 came from trail and error for this dataset.\n",
        "You have to find it for your face."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWelfmN9qghy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"..path_to_image_which_is_to_be_verify../img.jpg\"\n",
        "recog(path, face_database, model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}